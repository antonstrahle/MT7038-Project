---
title: "Project 4"
author: "Anton Stråhle & Max Sjödin"
date: "28 december 2020"
output: pdf_document
geometry: margin=2cm
---

# Introduction

This is a project for the course MT7038 which was given during the fall of 2020. In the project we'll examine some basic methods for binary classification that allow for regualarization through parameter choices such as SVMs and Logistic Regression.

# Data

We picked the `Occupancy Detection` data set as we wanted to work with a binary classification problem as this would allow us to apply most of the methodologies discussed throughout the course. As there are quite a few different binary data sets at UCI we specifically chose our data set as it had a sizable number of instances as well as  few, but intuitively and explanatory, features. The data set also poses an interesting question with how the features change over time and how this impacts classification.

## Attributes

The `Occupancy Detection` data set includes snapshots of a specific room every minute throughout the course of a few weeks. The aim is to classify the current `Occupancy` of the room using the five features, `Temperature`, `CO2`, `Humidity`, `HumidityRatio` and `Light`, which are observed each minute. The first three features are quite self-explanatory but the two final ones could use some clarification. The `Light` in the room is the light intensity, measured in Lux whilst the `HumidityRatio` is vaguely described as a "derived quantity from temperature and relative humidity, in kgwater-vapor/kg-air" by the contributors (and authors of *Candanedo et al. 2016*). 

## Exploration

From a quick overview we see that the data set is quite unbalanced.

```{r echo = FALSE, warning = FALSE, message = FALSE}

library(tidyverse)
library(e1071)
library(ggpubr)
library(readxl)
library(scales)
library(knitr)
library(psych)
library(kknn)
library(grid)
library(gridExtra)
library(caret)


d1 <- read.delim("../data/datatraining.txt", sep = ",")
d2 <- read.delim("../data/datatest.txt", sep = ",")
d3 <- read.delim("../data/datatest2.txt", sep = ",")

occupancyData <- d1 %>% 
  rbind(d2) %>% 
  rbind(d3) %>% 
  select(-date) %>% 
  mutate(Occupancy = factor(Occupancy))

sStats <- describeBy(occupancyData, occupancyData$Occupancy)

sStats$`0` %>% 
  knitr::kable(caption = "Unoccupied", digits = 3)

sStats$`1` %>% 
  knitr::kable(caption = "Occupied", digits = 3)

```

We have about four times more unoccupied than occupied minutes. As the data is observed around the clock it is of course natural that the room is unoccupied during a majority of the day. Due to this quite severe imbalance we initially wanted to make sure that our training, validation and testing sets reflected this inherent property of the data. However, as the data consists of minutely snapshots we have to make sure that each set consists of different, disjoint, time periods. The reason as to why we have to split the date into periods is that subsequent snapshots will very likely have the same feature values as well as occupancy status which would lead to a e.g. a 1-NN predicting with very higher accuracy accuracy (as almost identical data points would be present in both the training, validation and testing data), which is not really what we want. Due to the aforementioned issue with identical data points we will not be using cross validation and instead resort to using a dedicated validation set to evaluate any parameters.

The class imbalance turned out to be a real issues since some classifiers simply classified the room as unoccupied at all times. As such we resorted to upsample our training set in order to better allow for the learning of classification of both classes. Due to being outside the the scope of this project we simply generated observations of the minority class with replacement until we had an even split in the training data. In the end this proved to be a major success as we not only classified both classes but also improved the general performance of our models drastically.

We also standardize the data before upsampling as we will be using some classifiers that are not scale invariant.

We now examine boxplots of the data for each of the two classes.

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.width=13}

occupancyData %>% 
  gather(key = "Variable", value = "Value", -Occupancy) %>% 
  ggplot(aes(x = Occupancy, y = Value, color = Occupancy)) +
  geom_boxplot() +
  facet_wrap(~Variable, scales = "free_y") +
  theme(legend.position = "none") +
  ggtitle("Regular") -> p1

as.data.frame(occupancyData) %>%
  mutate_if(is.numeric, scale) %>% 
  gather(key = "Variable", value = "Value", -Occupancy) %>% 
  ggplot(aes(x = Occupancy, y = Value, color = Occupancy)) +
  geom_boxplot() +
  facet_wrap(~Variable, scales = "free_y") +
  ggtitle("Standardized") -> p2

grid.arrange(p1, p2, ncol = 2, top = textGrob("Figure 1: Boxplots",gp=gpar(fontsize=20,font=3)))
```

As can be seen in both the figure and the tables above the feature `Light` seems to to quite well in describing the current occupancy of the room. This is quite evident as people rarely gather in a room with the lights turned off, and (hopefully) turn the lights off when they leave. This features solely dominates the others when it comes to classification and is, at least according to us, quite boring. As such we'll choose to excluded it in order to actually be able to perform a somewhat comprehensive analysis that doesn't just include the question *Was the lights on or off?*.

```{r echo = FALSE, warning = FALSE, message = FALSE}

pData <- occupancyData %>% 
  select(-Light)

cols <- character(nrow(pData))

cols[as.numeric(as.character(pData$Occupancy)) == 0] <- "deepskyblue1"
cols[as.numeric(as.character(pData$Occupancy)) == 1] <- "coral2"

pairs(pData[,-5], col = scales::alpha(cols, 0.2), main = "Figure 2: Correlation between features")

```

As we can see in the figure above the remaining features seem to behave quite nicely, although some could be considered to be somewhat correlated (e.g. `Humidity` and `CO2` and obviously `HumidityRatio`). 

# Methodology

As we're dealing with a binary classification problem there are some different methods that we have decided to test out. We initially tested a variety of classifiers but quickly came to the conclusion that most generalized poorly in validation and testing scenarios. As such decided to focus on classifiers that we can tweak through validation such that they generalize well and perhaps ignore any slight differences between the different data sets (as previously mentioned the sets are a collection of snapshots from entirely different days so smaller differences are not unlikely). 

Furthermore we also decided to focus on regularization and generalization as we assumed that the cutoffs (i.e. the minutes after the occupancy status changes) would be very difficult to classify and as such we did not want a very specific classifier where these points would greatly impact the final classifier. The feature values take time to change (i.e. the `CO2` does not go to zero the minute that the room becomes unoccupied and neither does it jump to normal levels (for an occupied room) exactly when people enter) which should make the minutes before and after occupancy difficult to distinguish from actual occupation.

Due to the aforementioned reasons we have resorted to some very simple classifiers that we think will generalize well. Those specifically being SVM and regularized Logistic Regression.

## SVM

As support vector machines are good choices for classification it seems appropriate to apply the method to this problem. With a low cost $C$ we should be able to generalize well and thus achieve adequate testing accuracy even if the testing set might differ slightly from the training set. 

### Implementation

Using the package `e1071` and the function `svm` we implement a linear, polynomial and a radial SVM and use a coarse-to-fine search for a good value of $C$ using our aforementioned validation set. The results of this search can be found in Figure 1, 2 and 3 in the Appendix.

```{r echo = FALSE, message = FALSE, warning = FALSE, results = FALSE, cache = TRUE}
source("svm.R")

#Linear

lcDF1 <- valLinearSVM(10^seq(-5, 1, by = 1))
lcDF2 <- valLinearSVM(10^seq(-4.5, -3.5, by = 0.1))

lcDF <- lcDF1 %>% 
  rbind(lcDF2)

bestLC <- lcDF %>% 
  arrange(error) %>% 
  select(c) %>% 
  slice(1) %>% 
  pull()

ml <- svm(Occupancy ~ ., data = trainingData, kernel = "linear", cost = bestLC)

predL <- predict(ml, testingData)

testErrorLSVM <- mean(predL != testingData$Occupancy)

#Radial

rcDF1 <- valRadialSVM(10^seq(-5, -1, by = 1))
rcDF2 <- valRadialSVM(10^seq(-5, -4, by = 0.25)) #many give us the same validation error, i.e. as long as it is low enough

rcDF <- rcDF1 %>% 
  rbind(rcDF2) 

bestRC <- rcDF %>% 
  arrange(error) %>% 
  select(c) %>% 
  slice(1) %>% 
  pull()

mr <- svm(Occupancy ~ ., data = trainingData, kernel = "radial", cost = bestRC)

predR <- predict(mr, testingData)

testErrorRSVM <- mean(predR != testingData$Occupancy)

#Polynomial

pcDF1 <- valPolynomialSVM(10^seq(-5, 1, by = 1), 2:4, 1)
pcDF2 <- valPolynomialSVM(10^seq(-4.25, -3.25, by = 0.1), 2, 1)
pcDF3 <- valPolynomialSVM(10^seq(-4, -3, by = 0.1), 3, 1)
pcDF4 <- valPolynomialSVM(10^seq(-5, -4, by = 0.1), 4, 1)
pcDF5 <- valPolynomialSVM(10^seq(-4.6, -4.3, by = 0.05), 4, 1)

pcDF <- pcDF1 %>% 
  rbind(pcDF2) %>% 
  rbind(pcDF3) %>% 
  rbind(pcDF4) %>% 
  rbind(pcDF5)

bestP <- pcDF %>% 
  arrange(error) %>% 
  select(d, c) %>% 
  slice(1)

mp <- svm(Occupancy ~ ., data = trainingData, kernel = "polynomial", cost = bestP$c, degree = bestP$d, coef0 = 1)

predP <- predict(mp, testingData)

testErrorPSVM <- mean(predP != testingData$Occupancy)

```

We seem to have similar testing performances for all different kernels as can be seen in the table below.

```{r echo = FALSE, warning = FALSE, message = FALSE}

data.frame(Kernel = c("Linear", "Radial", paste("Polynomial Degree", bestP$d)),
           Cost = c(bestLC, bestRC, bestP$c),
           TestAccuracy = c(1-testErrorLSVM, 1-testErrorRSVM, 1-testErrorPSVM)) %>% 
  kable(caption = "SVM Test Accuracies", digits = 5)

```

We also note that the optimal values of $C$, based on the performance on the validation set, turned out to be very low for all three kernels. As we initially thought there does seem to be a need to be quite heavily regularized in order to perform well on the validation set.

We can also examine the confusion matrices for the three models in order to examine whether the misclassifications are split evenly between the classes or not.

```{r echo = FALSE, warning = FALSE, message = FALSE}

library(kableExtra)

lCM <- confusionMatrix(predL, testingData$Occupancy)

rCM <- confusionMatrix(predR, testingData$Occupancy)

pCM <- confusionMatrix(predP, testingData$Occupancy)

cl <- as.data.frame.matrix(lCM$table)
cr <- as.data.frame.matrix(rCM$table)
cp <- as.data.frame.matrix(pCM$table)

cSVM <- cbind(cl, cr, cp)

kable(cSVM, longtable =T, booktabs =T, caption ="Confusion Matrices") %>% 
  add_header_above(c(" ","Linear"=2,"Radial"=2,"Polynomial"=2))%>%
  kable_styling(latex_options =c("repeat_header")) %>% 
  kable_classic()

```

In all three models we seem to have proportionally fewer false negatives than we do false positives. This might be due to how the feature values change at the cutoff points (i.e. when the occupancy status changes). This would perhaps indicate that the change is more rapid when going from occupied to unoccupied than the other way around.  

In all it seems that low cost SMVs, regardless of kernel, are a good tool when stability is of the utmost importance (at least when it comes to our data set).

## Logistic Regression

Logistic regression is a good choice for binary classification problems like this one. We also look at different forms of regularized logistic regression as well as boosting to hopefully achieve higher test accuracies.

### Implementation

We use the package `mboost` and the function `glmboost` with the adaboost algorithm in order to boost the logistic regression model. For regularized logistic regression we look at both lasso and ridge regression using the package `glmnet` and the function `glmnet`. 

Unless the complexity parameter $\lambda$ is specified the function `glmnet` finds the optimal $\lambda$ based on the training set given. In an attempt to make the regularized logistic regression generalize better we use the validation set to manually find the $\lambda$ which optimizes performance on the aforementioned set. The $\lambda$ values and their corresponding validation accuracies can be found in Figure 4 and 5 in the Appendix.

Boosting algorithms weights observations based on classification, placing more weight on misclassified observations. For each iteration more focus is put on those observations previously missclassified. 

As mentioned under "Methodology" most of our observations the room is unoccupied with no changes from one minute to the next. A minority of observations, centered around the time were the room becomes unoccupied, are the ones hard to classify. We implement boosting in order to put more focus on this minority of hard-to-classify observations, which hopefully increases the test accuracy. 

Implementing these algorithms gave the following results.

```{r echo = FALSE, warning = FALSE, message = FALSE}
source("boosting.R")

data.frame(Method = c("Logit", "Boosting", "Lasso", "Ridge"),
           TestAccuracy = c(1-logit.test.error, 1-boost.test.error, 1-lasso.test.error, 1-ridge.test.error)) %>% 
          arrange(-TestAccuracy) %>%   
  kable(caption = "Logistic Regression Test Accuracies", digits = 5)
```

The simple logistic regression with no extra algorithms implemented resulted in slightly higher test accuracy than both boosting and regularized logistic regression. The algorithms implemented failed to improve the test accuracy and in the case of ridge regression even lowered the test accuracy significantly. This indicates that all features are important and that shrinking any of them negatively impacts classification.



# Discussion

When upsampling the minority class occupied to resolve the class imbalance in the data sets we saw a drastic increase in testing performance. Without the upsampling we achieved approximately $72\%$ accuracy for the linear SVM whilst we achieved $84\%$ with the upsampled training and validation data (other methods simply achieved an accuracy of about $65\%$ by classifying everything as unoccupied).

Although we have achieved decent classification accuracy we have to keep in mind that this is a binary classification problem and as such an accuracy of $50\%$ can be obtained by randomly guessing, assuming that the data is balanced. We think that many of the misclassifications are due to the fact that we have cutoff observations (these being the first few minutes at the start of an occupancy and a few minutes after occupancy). These observations are very difficult (if not impossible) to determine the class of as the feature values are extremely similar to the minutes before when the occupancy status was different. The way in which these have been distinguished in the past (that being in the paper *Candanedo et al. 2016* and in *Tütüncü et al. 2018*) is through the inclusion of the feature `Light`. As mentioned this variable was excluded in our case since it essentially explains the occupancy status by entirely by itself (a decision tree that only separates the classes by `Light` achieved an accuracy of about $98\%$).

# Improvements

There are numerous areas of improvements and other methods which we would have liked to examine. The upsampling that we perform had great impact on the performance of all our models. The method that we use is however very simple and there are several other, more sophisticated, ways of doing it which would likely lead to better training and performance.

We would have liked to examine a few different ensemble methods as it seems intuitive that these would further help with generalization. Then again there are of course many other methods that can be tuned to generalize well which we have not examined, but perhaps would have liked to.

It would also be of interest to examine which data points that were actually misclassified (e.g. if it actually were the cutoffs points that have been mentioned a few times throughout this project) and if the performance on these points specifically could be improved.

\newpage

# Bibliography

Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). *The elements of statistical learning: data mining, inference, and prediction.* 2nd ed. New York: Springer.

Candanedo, L. M., & Feldheim, V. (2016). *Accurate occupancy detection of an office room from light, temperature, humidity and CO 2 measurements using statistical learning models.* Energy and Buildings, 112, 28–39. doi:10.1016/j.enbuild.2015.11.071

Tütüncü, Kemal & çataltaş, Özcan & Koklu, Murat. (2018). *Occupancy Detection Through Light, Temperature, Humidity and CO2 Sensors Using ANN.* 

# Appendix

## Figures

```{r echo = FALSE, message = FALSE, warning = FALSE, fig.width=10, fig.height = 5}

lcDF %>% 
  ggplot(aes(x = c, y = error)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  ylab("Error on Validation Set") +
  xlab(expression(c)) +
  ggtitle("Figure 1: Linear SVM: Coarse-to-fine for C")

rcDF %>% 
  ggplot(aes(x = c, y = error)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  ylab("Error on Validation Set") +
  xlab(expression(c)) +
  ggtitle("Figure 2: Radial SVM: Coarse-to-fine for C")

pcDF %>% 
  ggplot(aes(x = c, y = error)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  ylab("Error on Validation Set") +
  xlab(expression(c)) +
  ggtitle("Figure 3: Polynomial SVM: Coarse-to-fine for C") +
  facet_wrap(~d)

lRidge %>% 
  ggplot(aes(x = l, y = error)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  ylab("Error on Validation Set") +
  xlab(expression(lambda)) +
  ggtitle("Figure 4: Ridge regression: Coarse for Lambda")

lLasso %>% 
  ggplot(aes(x = l, y = error)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  ylab("Error on Validation Set") +
  xlab(expression(lambda)) +
  ggtitle("Figure 5: Lasso regression: Coarse for Lambda")
```

\newpage

## Code

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```

\newpage

### data.r

Includes the initial manipulation of the data.

```{r, code = readLines('data.r'), eval = FALSE}

```

\newpage

### svm.r

Includes the functions for validating different svm models.

```{r, code = readLines('svm.r'), eval = FALSE}

```

\newpage

### boosting.r

Includes the code for anything related to logistic regression.

```{r, code = readLines('boosting.r'), eval = FALSE}

```