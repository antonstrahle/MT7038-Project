---
title: "Project 4"
author: "Anton Stråhle & Max Sjödin"
date: "28 december 2020"
output: pdf_document
geometry: margin=1cm
---

# Introduction

This is a project for the course MT7038 which was given during the fall of 2020. In the project we'll examine some basic methods for binary classification on a granular level and then proceed with a more in depth analysis and discussion for those that seem to perform best. The methods that we will initially test are LDA, Logistic Regression, Naive Bayes and Linear SVMs.

# Data

We picked the `Occupancy Detection` data set as we wanted to work with a binary classification problem as this would allow us to apply most of the methodologies discussed throughout the course. As there are quite a few different binary data sets at UCI we specifically chose our data set as it had a sizable number of instances as well as  few, but intuitively explanatory, features. The data set also poses an interesting question with how the features change over time and how this impacts classification.

## Attributes

The data `Occupancy Detection` data set includes snapshots of a specific room every minute throughout the course of a few weeks. The aim is to classify the current `Occupancy` of the room using the five features, `Temperature`, `CO2`, `Humidity`, `HumidityRatio` and `Light`, which are observed each minute. The first three features are quite self-explanatory but the two final ones could use some clarification. The `Light` in the room is the light intensity, measured in Lux whilst the `HumidityRatio` is vaguely described as a "derived quantity from temperature and relative humidity, in kgwater-vapor/kg-air". 

## Exploration

From a quick overview we see that the data set is quite unbalanced.

```{r echo = FALSE, warning = FALSE, message = FALSE}

library(tidyverse)
library(e1071)
library(ggpubr)
library(readxl)
library(scales)
library(knitr)
library(psych)
library(kknn)
library(grid)
library(gridExtra)
library(caret)

sStats <- describeBy(occupancyData, occupancyData$Occupancy)

sStats$`0` %>% 
  kable(caption = "Unoccupied", digits = 3)

sStats$`1` %>% 
  kable(caption = "Occupied", digits = 3)

```

We have about four times more unoccupied than occupied minutes. As the data is observed around the clock it is of course natural that the room is unoccupied during a majority of the day. Due to this quite severe imbalance we initially wanted to make sure that our training, validation and testing sets reflected this inherent property of the data. However, as the data consists of minutley snapshots we have to make sure that each set consists of different time periods. The reason as to why we have to split the date into periods is that subsequent snapshots will very likely have the same feature values as well as occupancy status which would lead to a e.g. a 1-NN predicting with 100% accuracy, which is not really what we want. Due to the aforementioned issue with identical data points we will not be using cross validation and instead resort to using a specific validation set to evaluate any parameters.

We have about four times more unoccupied than occupied minutes. As the data is observed around the clock it is of course natural that the room is unoccupied during a majority of the day. This turned out to be a real issues since some classifiers simply classified the room as unoccupied at all times as this was the case in the validation set. As such we resorted to upsample both our training and our validation set in order to allow for the classification of both classes. In the end this proved to be a major success as we not only classified both classes but also improved the general performance of our models drastically.

We also chose to standardize our features to allow for better performance of scale dependent classifiers. As some of the features include some very major deviations as can be seen in the figure below the choice to not standardize would surely impact the performance of these classifiers negatively. 

```{r echo = FALSE, warning = FALSE, message = FALSE, fig.width=13}

occupancyData %>% 
  gather(key = "Variable", value = "Value", -Occupancy) %>% 
  ggplot(aes(x = Occupancy, y = Value, color = Occupancy)) +
  geom_boxplot() +
  facet_wrap(~Variable, scales = "free_y") +
  theme(legend.position = "none") -> p1

as.data.frame(occupancyData) %>%
  mutate_if(is.numeric, scale) %>% 
  gather(key = "Variable", value = "Value", -Occupancy) %>% 
  ggplot(aes(x = Occupancy, y = Value, color = Occupancy)) +
  geom_boxplot() +
  facet_wrap(~Variable, scales = "free_y") -> p2

grid.arrange(p1, p2, ncol = 2)
```

As can be seen in both the figure and the tables above the feature `Light` seems to to quite well in describing the current occupancy of the room. This is quite evident as people rarely gather in a room with the lights turned off, and (hopefully) turn the lights off when they leave. This features solely dominates the others when it comes to classification and is, at least according to us, quite boring. As such we'll choose to excluded it in order to actually be able to perform a somewhat comprehensive analysis that doesn't just include the questions *Was the lights on or off?*.

```{r echo = FALSE, warning = FALSE, message = FALSE}

pData <- occupancyData %>% 
  select(-Light)

cols <- character(nrow(pData))

cols[as.numeric(as.character(pData$Occupancy)) == 0] <- "deepskyblue1"
cols[as.numeric(as.character(pData$Occupancy)) == 1] <- "coral2"

pairs(pData[,-5], col = scales::alpha(cols, 0.2), main = "Figure 2: Correlation between features")

```

As we can see in the figure above the remaining features seem to behave quite nicely, although some could be considered to be somewhat correlated (e.g. `Humidity` and `CO2`). 

# Methodology

As we're dealing with a binary classification problem there are some different methods that we have decided to test out. Initially we examined several non-linear classifiers as we believed that the data showed indications of being non-linear. However, it turned out that these generalized very poorly in validation scenarios which made us switch to more stable models. We performed this switch as we assumed that the data we could only support a simple decision boundary as the cutoffs (i.e. the minutes after the occupancy status changes) should be very difficult to classify. The feature values take time to change (i.e. the CO2 does not go to zero the minute that the room becomes unoccupied and neither does it jump to normal levels exactly when people enter).

Due to the aforementioned reasons we have resorted to some very simple, but stable, classifiers. Those specifically being LDA, Naive Bayes, Logistic Regerssion and Linear SVM. 

## SVM

As support vector machines are good choices for classification it seems appropriate to apply the method to this problem. With a low cost $C$ we should be able to generalize well and thus achieve adequate testing accuracy.

### Implementation

Using the package `e1071` and the function `svm` we implement a linear, polynomial and a radial SVM and use a coarse-to-fine search for a good value of $C$ using our aforementioned validation set. 

```{r echo = FALSE, message = FALSE, warning = FALSE, results = FALSE, cache = TRUE}
source("svm.R")

#Linear

lcDF1 <- valLinearSVM(10^seq(-5, 1, by = 1))
lcDF2 <- valLinearSVM(10^seq(-4.5, -3.5, by = 0.1))

lcDF <- lcDF1 %>% 
  rbind(lcDF2)

bestLC <- lcDF %>% 
  arrange(error) %>% 
  select(c) %>% 
  slice(1) %>% 
  pull()

ml <- svm(Occupancy ~ ., data = train, kernel = "linear", cost = bestLC)

predL <- predict(ml, testingData)

testErrorLSVM <- mean(predL != testingData$Occupancy)

#Radial

rcDF1 <- valRadialSVM(10^seq(-5, 1, by = 0.25))
rcDF2 <- valRadialSVM(10^seq(-2.5, -1, by = 0.1))

rcDF <- rcDF1 %>% 
  rbind(rcDF2) 

bestRC <- rcDF %>% 
  arrange(error) %>% 
  select(c) %>% 
  slice(1) %>% 
  pull()

mr <- svm(Occupancy ~ ., data = train, kernel = "radial", cost = bestRC)

predR <- predict(mr, testingData)

testErrorRSVM <- mean(predR != testingData$Occupancy)

#Polynomial

pcDF1 <- valPolynomialSVM(10^seq(-4, 1, by = 0.5), 2:4, 1)
pcDF2 <- valPolynomialSVM(10^seq(-3, -2, by = 0.1), 2:3, 1)
pcDF3 <- valPolynomialSVM(10^seq(-4, -3, by = 0.1), 4, 1)

pcDF <- pcDF1 %>% 
  rbind(pcDF2) %>% 
  rbind(pcDF3)

bestPC <- pcDF %>% 
  arrange(error) %>% 
  select(d, c) %>% 
  slice(1)

mp <- svm(Occupancy ~ ., data = train, kernel = "radial", cost = bestPC$c, degree = bestPC$d, coef0 = 1)

predP <- predict(mp, testingData)

testErrorPSVM <- mean(predP != testingData$Occupancy)

```


```{r echo = FALSE, warning = FALSE, message = FALSE}

data.frame(Kernel = c("Linear", "Radial", paste("Polynomial Degree", bestP$d)),
           Cost = c(bestLC, bestRC, bestP$c),
           TestAccuracy = c(1-testErrorLSVM, 1-testErrorRSVM, 1-testErrorPSVM)) %>% 
  kable(caption = "SVM Test Accuracies", digits = 3)

```

We also note that the optimal values of $C$, based on the performance on the validation set, turned out to be very low for all three kernels.

This vast difference between validation and test performance must surely be attributed to differences in the number of occupied/unoccupied observations in each of the sets. As such we examine the confusion matrices for the testing data for our three models.

```{r echo = FALSE, warning = FALSE, message = FALSE}

lCM <- confusionMatrix(testingData$Occupancy, predL)

rCM <- confusionMatrix(testingData$Occupancy, predR)

pCM <- confusionMatrix(testingData$Occupancy, predP) 


```


```{r}

source("data.r")

nbTrain <- trainingData %>% select(-HumidityRatio)
nbTest <- testingData %>% select(-HumidityRatio)

m <- naiveBayes(Occupancy ~ ., data = nbTrain)

pred <- m %>% predict(testingData)

testError <- mean(pred != testingData$Occupancy)

```

```{r}

source("data.R")

m <- glm(Occupancy ~., family="binomial", data= trainingData)

probs <- m %>% predict(testingData)

pred <- ifelse(probs > 0.5, 1, 0)

testError <- mean(pred != testingData$Occupancy)

```


For the linear kernel we only resort to a coarse parameter search as it's performance did not come close to being able to match that of the polynomial or the radial kernel as can be seen in Figure X in the Appendix.

For all kernels we encountered some computational difficulties (the optimizer did not converge within the iteration limit of the `e1071`) when examining higher costs or combinations of degrees and costs.

For the polynomial kernel we did note an increase in the validation accuracy when increasing both the degree and the cost (although we could not examine higher costs/degrees due to the aforementioned computational issues) which can be seen in Figure X in the Appendix. 

For the radial kernel we had similar findings as we did for the polynomial kernel. We achieved higher validation accuracies by increasing the cost (as can be seen in Figure X in the Appendix) which prompted us to once again disregard the finer part of the parameter search in order to simply examine higher costs.

```{r echo = FALSE, message = FALSE, warning = FALSE}

data.frame(Kernel = c("Linear", "Radial", paste("Polynomial Degree", bestP$D)),
           Cost = c(bestLC, bestRC, bestP$C),
           TestAccuracy = c(lAcc, rAcc, pAcc)) %>% 
  kable(caption = "SVM Test Accuracies", digits = 3)

```

For all the kernels we seem to favor a very hard margin as is evident by the increased validation performance when increasing the cost. This leads us to believe that the data is not very noisy as such hard margins would otherwise perform catastrphically on new test data, which it does not.

## KNN

As noted previously the data seems to be quite non-linear and as KNN in a good non-linear classifier (*Paul 2017*). As noted when examining the SVMs in the previous section we seem to favor low bias classifiers due to an inherent lack of noise in our data set which should indicate that we want a smaller $k$. Furthermore as KNN scales well with data we should hopefully be able to achieve quite good results given the size of our training set (approximately 12000 observations).

### Implementation

Using the package `Class` and the function `knn` we do a parameter search using our validation set for a good $k$ on $(1,...,50)$. 

```{r echo = FALSE, message = FALSE, warning = FALSE, results = FALSE}

set.seed(2021) #respecify seed to be able to run chunks alone

source("knn.R")

knnDF <- valKNN(seq(1, 400, by = 10))

bestK <- knnDF %>% 
  arrange(error) %>% 
  select(k) %>% 
  slice(1) %>% 
  pull()

k <- knn(trainingData[, !sapply(trainingData, is.factor)], testingData[, !sapply(testingData, is.factor)], 
         cl = trainingData[,sapply(trainingData, is.factor)], k = bestK)

kAcc <- mean(testingData[, sapply(testingData, is.factor)] == k)

```

The value of $k$ which generates the best validation accuracy turns out to be 1. As the best kNN classifier, which performs extremely well with a test accuracy of `r kAcc` is a 1-NN, this strongly indicates that our data is not very noisy at all (which we also noted when examining the SVMs). 

A possible way to enhance the nearest neighbor model is through weighting (*Hastie et al. 2009*). A simple weighting scheme would be to weight the k nearest neighbors by their distance to the new point we want to classify, as such we give further emphasis to neighbors closer to the point which we wish to classify. 

In order to implement this weighting scheme we use the `kknn` package and the included function with the same name which uses kernel-difference weighting. The package allows for the usage of many different kernels but the results that they produce do not differ enough to justify an inclusion of all of them. As such we use the *Gaussian* kernel as it is one which we've encountered before.  

```{r echo = FALSE, message = FALSE, warning = FALSE, results = FALSE}

set.seed(2021) #respecify seed to be able to run chunks alone

source("wknn.R")

wknnDF <- valWKNN(seq(400, 600, by = 25))

bestWK <- wknnDF %>% 
  arrange(error) %>% 
  select(k) %>% 
  slice(1) %>% 
  pull()

wk <- kknn(Occupancy ~ ., train = trainingData, test = testingData, kernel = "gaussian", k = bestWK) 
    
wkAcc <- mean(testingData[, sapply(testingData, is.factor)] == wk$fitted.values)

```

When using a weighted `r bestWK`-NN we obtained a testing accuracy of `r wkAcc`. If we examine the parameter serach using the validation set in the Appendix we note that we essentially have the same validation error for a lot of different values of $k$. This is likely due to the fact that we have minutley observations that have the same occupancy status with equal, or very similar feature values which will lead to similar results for different values of $k$. In hindsight this similairity in the data set should imply that the usage of weighting is redundant as we would more often than not be weighting "identical" neighbors. 

## Decions Trees

Using decision trees is a good idea since we have non-linear data and a simple binary classification problem. 

### Implementation

```{r echo = FALSE, message = FALSE, warning = FALSE}
formula <- Occupancy ~ .

#Basic decision tree
tree <- rpart(formula, data = trainingData, method = "class")
treepred <- predict(tree, testingData, type = "class")
testErrorTree <- mean(sqrt((as.numeric(as.character(treepred)) - as.numeric(as.character(testingData$Occupancy)))^2))

```

Constructing a simple decision tree resulted in a test accuracy of `r 1-testErrorTree`. This is can be further optimized by implementing bagging and boosting algorithms. 

Bagging procedures draw a predetermined number of bootstrap samples each fitting a model. The models outputted predictions are averaged and gives the resulting bagged estimate for each observation. 

Boosting procedures train the classifier by weighting each observation based on its classification, placing more weight on observations incorrectly classified. The next iteration of the procedure focuses more on those previously misclassified observations to better classify the training data.
Using the package `ipred` for bagging and the package `adabag` for boosting we attained the following results.

```{r echo = FALSE, message = FALSE, warning = FALSE}

#Bagging
bagged <- ipred::bagging(formula, data = trainingData, nbagg = 1000, coob = TRUE)
#Out-of-bag error

bagpred <- predict(bagged, newdata = testingData)

testErrorBag <- mean(sqrt((as.numeric(as.character(bagpred)) - as.numeric(as.character(testingData$Occupancy)))^2))

#Boosting
#Boosting 1000 trees resulted in long computation time
boosttree <- boosting(formula, data = trainingData, mfinal = 100)

boostpred <- predict.boosting(boosttree, newdata = testingData)

testErrorBoost <- boostpred$error

data.frame(Method = c("Simple", "Bagging", "Boosting"),
           'Test Accuracy' = c(1-testErrorTree, 1-testErrorBag, 1-testErrorBoost)) %>% 
  kable(caption = "Decision Trees", digits = 3)

```

In the case of bagging we draw $1000$ bootstrap samples and in the case of boosting we iterate over $100$ trees. For boosting, iteration over $1000$ trees led to long computational time and did not change the result significantly. From Table 4 we can conclude that bagging resulted in higher Test Accuracy than boosting. Although bagging resulted in the highest Test Accuracy we loose the interpretability of the tree based structure, since a bagged tree no longer is a tree (*Hastie et al. 2009*). If interpretability is valued, the boosting procedure is worth considering despite the lower Test Accuracy.

# Discussion

The different methods which we've examined throughout this project yield the following results when each of them have been tuned (although to different degrees).

```{r echo = FALSE, warning = FALSE, message = FALSE}

data.frame(Method = c("Bagging", paste0("Weighted ", bestWK, "-NN"), "Radial SVM"),
           'Accuracy' = c(1-testErrorBag, wkAcc, rAcc)) %>% 
  arrange(desc(Accuracy)) %>% 
  kable(caption = "Final Test Accuracies", digits = 3)

data.frame(Method = c("Simple","Boosting","Bagging", paste0("Weighted ", bestWK, "-NN"), "Radial SVM","Linear SVM", paste("Polynomial SVM Degree", bestP$D)),
           'Accuracy' = c(1-testErrorTree, 1-testErrorBoost,1-testErrorBag, wkAcc, rAcc, lAcc, pAcc)) %>% 
  arrange(desc(Accuracy)) %>% 
  kable(caption = "Final Test Accuracies", digits = 3)
```

Bagging decision trees resulted in the highest test accuracy, closely followed by the weighted `r bestWK`-NN.


# Bibliography

Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). *The elements of statistical learning: data mining, inference, and prediction.* 2nd ed. New York: Springer.

Paul, M. (2017), *Nonlinear Classification* [Presentation]. \\
https://cmci.colorado.edu/classes/INFO-4604/fa17/files/slides-9_nonlinear.pdf 

Accurate occupancy detection of an office room from light, temperature, humidity and CO2 measurements using statistical learning models. Luis M. Candanedo, VÃ©ronique Feldheim. Energy and Buildings. Volume 112, 15 January 2016, Pages 28-39

# Appendix

## Figures

```{r echo = FALSE, message = FALSE, warning = FALSE}

knnDF %>% 
  ggplot(aes(x = k, y = error)) +
  geom_line() +
  geom_point() +
  ylab("Error on Validation Set") +
  xlab(expression(k)) +
  ggtitle("KNN Tuning")

wknnDF %>% 
  ggplot(aes(x = k, y = error)) +
  geom_line() +
  geom_point() +
  ylab("Error on Validation Set") +
  xlab(expression(k)) +
  ggtitle("Weighted-KNN Tuning")

```


## Code

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```


```{r echo=TRUE, eval=FALSE}
#svm.R
source("data.R")



valLinearSVM <- function(sequence, train = trainingData, val = validationData){
  
  bestCost <- 0
  bestValError <- 1
  
  for(C in sequence){
    
    m <- svm(Occupancy ~ ., data = train, kernel = "linear", cost = C)
    pred <- predict(m, val)
    
    valError <- mean(pred != val$Occupancy)
    
    #print(paste("Training Error:", round(mean(m$fitted != train$Occupancy), 4), "for C:", C)) 
    print(paste("Validation Error:", round(valError, 4), "for C:", C)) 
    
    if(valError < bestValError){
      
      bestValError <- valError
      bestCost <- C
      
    }
    
  }
  
  bestCost
  
}

valRadialSVM <- function(sequence, train = trainingData, val = validationData){
  
  bestCost <- 0
  bestValError <- 1
  
  for(C in sequence){
    
    m <- svm(Occupancy ~ ., data = train, kernel = "radial", cost = C)
    pred <- predict(m, val)
    
    valError <- mean(pred != val$Occupancy)
    
    #print(paste("Training Error:", round(mean(m$fitted != train$Occupancy), 4), "for C:", C)) 
    print(paste("Validation Error:", round(valError, 4), "for C:", C)) 
    
    if(valError < bestValError){
      
      bestValError <- valError
      bestCost <- C
      
    }
    
  }
  
  bestCost
  
}

valPolynomialSVM <- function(C, deg, coeff, train = trainingData, val = validationData){
  
  bestCost <- 0
  bestDeg <- 0
  bestValError <- 1
  
  for(d in deg){  
    
    for(c in C){
    
      m <- svm(Occupancy ~ ., data = train, kernel = "polynomial", cost = c, degree = d, coef0 = coeff)
      pred <- predict(m, val)
      
      valError <- mean(pred != val$Occupancy)
      
      #print(paste("Training Error:", round(mean(m$fitted != train$Occupancy), 4), "for C:", c)) 
      print(paste("Validation Error:", round(valError, 4), "for C:", c, "and D:", d)) 
      
      if(valError < bestValError){
        
        bestValError <- valError
        bestCost <- c
        bestDeg <- d
        
      }
      
    }
  
  }
  
  list("C" = bestCost, "D" = bestDeg)
  
}
```

```{r echo=TRUE, eval=FALSE}
#knn.R
source("data.R")
library(class)

valKNN <- function(K, train = trainingData, val = validationData){
  
  bestK <- 0
  bestError <- 1
  
  for(k in K){
  
    m <- knn(train[, !sapply(train, is.factor)], val[, !sapply(val, is.factor)], cl = train[,sapply(train, is.factor)], k = k) 
    
    valError <- mean(val[, sapply(val, is.factor)] != m)
    
    print(paste("Validation Error:", round(valError, digits = 3), "for k =", k))
    
    if(valError < bestError){
      
      bestK <- k
      bestError <- valError
      
    }
  
  }
  
  bestK
    
}

```

```{r echo=TRUE, eval=FALSE}
#wknn.R
source("data.r")
library(kknn)

valWKNN <- function(K, kernel = "gaussian", train = trainingData, val = validationData){
  
  bestK <- 0
  bestError <- 1
  
  for(k in K){
    
    m <- kknn(Occupancy ~ ., train = train, test = val, kernel = kernel, k = k) 
    
    valError <- mean(val[, sapply(val, is.factor)] != m$fitted.values)
    
    print(paste("Validation Error:", round(valError, digits = 3), "for k =", k))
    
    if(valError < bestError){
      
      bestK <- k
      bestError <- valError
      
    }
    
  }
  
  bestK
  
}

```

```{r echo=TRUE, eval=FALSE}
#data.R
library(tidyverse)
library(e1071)
library(ggpubr)
library(readxl)
library(scales)
library(rpart)
library(rpart.plot)

#occupancy Dataset

d1 <- read.delim("../data/datatraining.txt", sep = ",")
d2 <- read.delim("../data/datatest.txt", sep = ",")
d3 <- read.delim("../data/datatest2.txt", sep = ",")

occupancyData <- d1 %>% 
  rbind(d2) %>% 
  rbind(d3) %>% 
  select(-date) %>% 
  mutate(Occupancy = factor(Occupancy)) %>% 
  select(-Light)

#From CrossValidate package (issues with dependencies in the install so I yoinked their source code)
balancedSplit <- function(fac, size){
  trainer <- rep(FALSE, length(fac))
  for(lev in levels(fac)){
    N <- sum(fac==lev)
    wanted <- max(1, trunc(N*size))
    trainer[fac==lev][sample(N, wanted)] <- TRUE
  }
  trainer
}

train <- balancedSplit(occupancyData$Occupancy, size = 0.6)

rawTrainingData <- occupancyData[train,]  
remainingData <- occupancyData[!train,]

validation <- balancedSplit(remainingData$Occupancy, 0.5)

rawValidationData <- remainingData[validation,]
rawTestingData <- remainingData[!validation,]

standardizeData <- function(data, rawTrain = rawTrainingData){
  
  attr <- rawTrain[,!sapply(rawTrain, is.factor)]
  
  mean <- apply(attr, 2, mean)
  sd <- apply(attr, 2, sd)
  
  data.frame(Occupancy = data[,sapply(rawTrain, is.factor)]) %>% 
    cbind(t((t(data[,!sapply(rawTrain, is.factor)]) - mean)/sd))
  
}

#Standardized separately, should be the same mean and sd for all data
trainingData <- standardizeData(rawTrainingData)
validationData <- standardizeData(rawValidationData)
testingData <- standardizeData(rawTestingData)

#Looks very nice id say

trainingData %>% 
  gather(key = "Variable", value = "Value", -Occupancy) %>% 
  ggplot(aes(x = Occupancy, y = Value, color = Occupancy)) +
  geom_boxplot() +
  facet_wrap(~Variable, scales = "free_y")

```
