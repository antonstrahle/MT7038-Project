---
title: "Project 4"
author: "Anton Stråhle & Max Sjödin"
date: "28 december 2020"
output: pdf_document
geometry: margin=1cm
---

# Introduction

This is a project for the course MT7038 which was given during the fall of 2020. In the project we'll examine some basic methods for binary classification on a granular level and then proceed with a more in depth analysis and discussion for those that seem to perform best. The methods that we will initially test  are SVM, KNN and Decision Trees.

# Data

We picked the `Occupancy Detection` data set as we wanted to work with a binary classification problem as this would allow us to apply most of the methodologies discussed throughout the course. As there are quite a few different binary data sets at UCI we specifically chose our data set as it had a sizeable number of instances as well as  few, but intuitivley explanatory, features.

## Attributes

The data `Occupancy Detection` data set includes snapshots of a specific room every minute throughout the course of a few weeks. The aim is to classify the current `Occupancy` of the room using the five features, `Temperature`, `CO2`, `Humidity`, `HumidityRatio` and `Light`, which are observed each minute. The first three features are quite self-explanatory but the two final ones could use some clarification. The `Light` in the room is the light intensity, measured in Lux whilst the `HumidityRatio` is vaguely described as a "derived quantity from temperature and relative humidity, in kgwater-vapor/kg-air". 

## Exploration

From a quick overview we see that the data set is quite unbalanced.

```{r echo = FALSE, warning = FALSE, message = FALSE}

library(tidyverse)
library(e1071)
library(ggpubr)
library(readxl)
library(scales)
library(rpart)
library(rpart.plot)
library(knitr)
library(psych)
library(ipred)
library(adabag)
library(gbm)
library(kknn)

set.seed(2021) #Happy New Year

d1 <- read.delim("../data/datatraining.txt", sep = ",")
d2 <- read.delim("../data/datatest.txt", sep = ",")
d3 <- read.delim("../data/datatest2.txt", sep = ",")

occupancyData <- d1 %>% 
  rbind(d2) %>% 
  rbind(d3) %>% 
  select(-date) %>% 
  mutate(Occupancy = factor(Occupancy))

sStats <- describeBy(occupancyData, occupancyData$Occupancy)

sStats$`0` %>% 
  kable(caption = "Unoccupied", digits = 3)

sStats$`1` %>% 
  kable(caption = "Occupied", digits = 3)

```

We have about four times more unoccupied than occupied minutes. As the data is observed around the clock it is of course natural that the room is unoccupied during a majority of the day. Due to this quite severe inbalance we have to make sure that our training, validation and testing sets reflect this inherent property of the data. As such we decided to concatenate the three provided data sets (training, validation and testing) and split these up into balanced sets ourselves as the data providers seems to have just split the complete date by the timestamp which leads to a quite severe inbalance as a weekend is included (i.e. no Occupied observations for two days).

Beyond the poor splitting of the data into training, validation and testing we found not obvious inconsitensies in the data that needed addressing.

We also chose to standardize to allow for better performance of scale dependent classifiers, e.g. kNN or SVM. As some of the features include some very major deviations as can be seen in the figure below this choice to not standardize would surely impact the performance of these classifiers negativley.

```{r echo = FALSE, warning = FALSE, message = FALSE}

occupancyData %>% 
  gather(key = "Variable", value = "Value", -Occupancy) %>% 
  ggplot(aes(x = Occupancy, y = Value, color = Occupancy)) +
  geom_boxplot() +
  facet_wrap(~Variable, scales = "free_y")

```

As can be seen in both the figure and the tables above the feature `Light` seems to to quite well in describing the current occupancy of the room. This is quite evident as people rarely gather in a room with the lights turned off, and (hopefully) turn the lights off when they leave. This features solely dominates the others when it comes to classification and is, at least according to us, quite boring. As such we'll choose to excluded it in order to actually be able to perform a somewhat comprehensive analysis that doesn't just include the questions *Was the lights on or off?*.

```{r echo = FALSE, warning = FALSE, message = FALSE}

pData <- occupancyData %>% 
  select(-Light)

cols <- character(nrow(pData))

cols[as.numeric(as.character(pData$Occupancy)) == 0] <- "deepskyblue1"
cols[as.numeric(as.character(pData$Occupancy)) == 1] <- "coral2"

pairs(pData[,-5], col = scales::alpha(cols, 0.2), main = "Figure 1: Correlation between features")

```

As we can see in the figure above the remaining features seem to behave quite nicely, although some could be considered to be somewhat correlated (e.g. `Humidity` and `CO2`). There seem to be some quite nice divisions of the occupancy status within the different features which should make adequate classification quite easy.

# Methodology

As we're dealing with a binary classification problem there are some different methods that we’ve decided to test out. The methods which we have chosen are SVM, KNN and Decision Trees. Our idea is to examine all methods quite shallowly and then go a bit deeper for one or two that shows promise.

## SVM

As support vector machines are widely used in classification (SOURCE) it seems appropriate to apply the method to this problem. In Figure 2 above we note that we seem to have non-linear data which suggest that either a polynomial or a radial kernel should work well, or at least better than a linear kernel.

### Implementation

We examine three different kernels, a linear kernel, a radial kernel as well as a polynomial kernel. We use our aforementioned validation set in order to perform a coarse-to-fine parameter search for the cost $C$ and the degree for the polynomial kernel (in the case of the polynomial kernel we use a grid coarse grid search and then a finer search for the best degree). Using the package `e1071` and the function `svm` we attained the following results for the three scenarios.


```{r echo = FALSE, message = FALSE, warning = FALSE, results = FALSE, cache = TRUE}

source("svm.R")

#Linear

bestLC <- valLinearSVM(10^seq(-2, 2, by = 0.5))

l <- svm(Occupancy ~ ., data = trainingData, kernel = "linear", C = bestLC)
predL <- predict(l, testingData)
lAcc <- mean(predL == testingData$Occupancy)

#Radial

bestRC <- valRadialSVM(10^seq(-2, 5, by = 0.5))

r <- svm(Occupancy ~ ., data = trainingData, kernel = "radial", C = bestRC)
predR <- predict(r, testingData)
rAcc <- mean(predR == testingData$Occupancy)

#Polynomial

bestP <- valPolynomialSVM(10^seq(-2, 3, by = 0.5), deg = seq(2,3), coeff = 1)

p <- svm(Occupancy ~ ., data = trainingData, kernel = "polynomial", C = bestP$C, degree = bestP$D, coef0 = 1)
predP <- predict(p, testingData)
pAcc <- mean(predP == testingData$Occupancy)

```

For the linear kernel we only resort to a coarse parameter search as it's performance is not going to be able to match that of the polynomial or the radial kernel (and since the validation error is the same it did not change much over the costs examined ($\{10^{i}\}_{i=-2}^3$).

For the polynomial kernel we encountered some computational difficulties when tuning our paremeters. As such we were only able to efficiently test up until degree 3. We did note an increase in the validation accuracy when increasing both the degree and the cost. This seems to indicate that the data lacks a lot of noise as we seem to favor a low bias, but high variance, classifier which we obtain by using such a high cost. 

For the radial kernel we had similar findings as we did for the polynomial kernel. We achieved higher validation accuracies by increasing the cost (this increase did however slow down eventually) which prompted us to once again disregard the finer part of the parameter search in order to simply examine higher costs.

```{r echo = FALSE, message = FALSE, warning = FALSE}

data.frame(Kernel = c("Linear", "Radial", paste("Pylynomial degree", bestP$D)),
           Cost = c(bestLC, bestRC, bestP$C),
           TestAccuracy = c(lAcc, rAcc, pAcc)) %>% 
  kable(caption = "SVM Test Accuracies", digits = 3)

```

## KNN

As noted previously the data seems to be quite non-linear and as KNN in a good non-linear classifier (SOURCE). As noted when examining the SVMs in the previous section we seem to favor low bias classifiers due to an inherent lack of noise in our data set which should inidcate that we want a smaller $k$. Furthermore as KNN scales well with data we should hopefully be able to achieve quite good results given the size of our training set (approximatley 12000 observations).

### Implementation

Using the package `Class` and the function `knn` we do a parameter search using our validation set for a good $k$ on $(1,...,50)$. 

```{r echo = FALSE, message = FALSE, warning = FALSE, results = FALSE}

source("knn.R")

bestK <- valKNN(1:50)

k <- knn(trainingData[, !sapply(trainingData, is.factor)], testingData[, !sapply(testingData, is.factor)], 
         cl = trainingData[,sapply(trainingData, is.factor)], k = bestK)

kAcc <- mean(testingData[, sapply(testingData, is.factor)] == k)

```

The value of $k$ which generates the best validation accuracy turns out to be 1. As the best kNN classifier, which performs extremely well with a test accuracy of `r kAcc` is a 1-NN, this strongly indicates that our data is not very noisy at all (which we also noted when examining the SVMs). 

A possible way to enhance the nearest neighbour model is through weighting (SOURCE BOOK). A simple weighting scheme would be to weight the k nearest neighbours by their distance to the new point we want to classify, as such we give further emphesis to neightbours closer to the point which we wish to classify. 

In order to implement this weighting scheme we use the `kknn` package and the included function with the same name which uses kernel-difference weighting. The package allows for the usage of many different kernels but the results that they produce do not differ enough to justify an inclusion of all of them. As such we use the so called *Epanechnikov* kernel as it is one which we've encountered before.  

```{r echo = FALSE, message = FALSE, warning = FALSE, results = FALSE}
source("wknn.R")

bestK <- valWKNN(1:25)

wk <- kknn(Occupancy ~ ., train = trainingData, test = testingData, kernel = "epanechnikov", k = bestK) 
    
wkAcc <- mean(testingData[, sapply(testingData, is.factor)] == wk$fitted.values)

```

When using weighting we actually use `r bestK` neighbours instead of only one, this also leads to a much higher accuracy as well at `r wkAcc`.

## Decions Trees

Using decision trees is a good idea since we have non-linear data and a simple binary classification problem. 

### Implementation

```{r echo = FALSE, message = FALSE, warning = FALSE}
formula <- Occupancy ~ .

#Basic decision tree
tree <- rpart(formula, data = trainingData, method = "class")
treepred <- predict(tree, testingData, type = "class")
testErrorTree <- mean(sqrt((as.numeric(as.character(treepred)) - as.numeric(as.character(testingData$Occupancy)))^2))

```

Constructing a simple decision tree resulted in a test accuracy of `r 1-testErrorTree`. This is can be further optimized by implementing bagging and boosting algorithms. 

Bagging procedures draw a predetermined number of bootstrap samples each fitting a model. The models outputted predictions are averaged and gives the resulting bagged estimate for each observation. 

Boosting procedures train the classifier by weighting each observation based on its classification, placing more weight on observations incorrectly classified. The next iteration of the procedure focuses more on those previously misclassified observations to better classify the training data.
Using the package `ipred` for bagging and the package `adabag` for boosting we attained the following results.

```{r echo = FALSE, message = FALSE, warning = FALSE}

#Bagging
bagged <- ipred::bagging(formula, data = trainingData, nbagg = 1000, coob = TRUE)
#Out-of-bag error

bagpred <- predict(bagged, newdata = testingData)

testErrorBag <- mean(sqrt((as.numeric(as.character(bagpred)) - as.numeric(as.character(testingData$Occupancy)))^2))

#Boosting
#Boosting 1000 trees resulted in long computation time
boosttree <- boosting(formula, data = trainingData, mfinal = 100)

boostpred <- predict.boosting(boosttree, newdata = testingData)

testErrorBoost <- boostpred$error

data.frame(Method = c("Simple", "Bagging", "Boosting"),
           'Test Accuracy' = c(1-testErrorTree, 1-testErrorBag, 1-testErrorBoost)) %>% 
  kable(caption = "Decision Trees", digits = 3)

```

In the case of bagging we draw $1000$ bootstrap samples and in the case of boosting we iterate over $100$ trees. For boosting, iteration over $1000$ trees led to long computational time and did not change the result significantly. From Table 4 we can conclude that bagging resulted in higher Test Accuracy than boosting. Although bagging resulted in the highest Test Accuracy we loose the interpretability of the tree based structure, since a bagged tree no longer is a tree (The Elements of Statistical Learning, second edition, s. 286). If interpretability is valued, the boosting procedure is worth considering despite the lower Test Accuracy.

# Discussion

The different methods which we've examined throughout this project yield the following results when each of them have been tuned (although to different degrees).

```{r echo = FALSE, warning = FALSE, message = FALSE}

data.frame(Method = c("Bagging", paste0("Weighted ", bestK, "-NN"), "Radial SVM"),
           'Accuracy' = c(1-testErrorBag, wkAcc, rAcc)) %>% 
  arrange(desc(Accuracy)) %>% 
  kable(caption = "Final Test Accuracies", digits = 3)

```

Bagging decision trees resulted in the highest test accuracy, closely followed by the weighted `r bestK`-NN.


# Bibliography

# Appendix
